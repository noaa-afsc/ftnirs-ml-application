---
title: Self hosting the application
---

The application uses python Dash, which is a full python frontend/backend framework. Dash is built
on top of flask, which is a common python backend archetecture for web applications usually paired with javascript as a front end. 

The applications needs to be hosted on a server that can process the dynamic content associated with model training. This means that self-hosting is necessary on a dedicated server- serverless web solutions for static content (like this github page) do not allow for the computationally intensive work the application is doing. 

There are a lot of potential options to host the application, and we will go into the important areas if self-hosting.  

* Host

The Dash application and other elements of the software stack (docker, web server) need to be on a dedicated host. Since the currently used technique is not GPU limited, it does not require specialized hardware, but performs better on machines with 8+ cores CPU and 64GB+ RAM, particularly if concurrent use from multiple users is expected. The host could be on cloud or a local network, but just needs to be accesible through http or https from personal computers of the users.  

* Web server: 

Dash needs to be hosted by a web server which can listen for https requests, forward them to the application, and reply to the user with https content. We are currently using NGINX for this purpose, which is the webserver framework used in ~30% of sites on the broader internet. Other popular alternatives for this include Traefik and Apache HTTP server, among many others. 

The web server can use the domain name if one is available or a subdomain or page of that domain. Note that Dash also needs knowledge of the URL (subdomain or pages) being served, and for this I use an environmental variable so that the domain values given to Dash NGINX remain identical.  

```python
app = Dash(__name__,requests_pathname_prefix=os.getenv('APPNAME'),routes_pathname_prefix=os.getenv('APPNAME'))
```

* Docker:

Docker is not explictly necessary, but helps simplify and atomize the software stack to make each part (webserver and dash application) more manageable. Docker compose is a function of CLI (command line) docker which allows you to define multiple containers in the same application, which can pull from latest published images (meaning, you don't have to maintain the web server application, just pull the off the shelf application) for any component parts of the application. Docker compose services are defined in a yaml file which instructs the engine how to build the application with containers from local sources or on the web. 

* Cloud storage

The application uses GCP storage buckets. Across cloud vendors, these are known as 'S3' type storage after the popular AWS equivalent service. The application is currently hardcoded to utilize GCP storage buckets, and so a lift-and-shift self-hosted version would also require a GCP project with the necessary storage buckets configured if it did not want to share data and models with the NMFS hosted application. Indeed, even the GCP project is currently hardcoded but that is a trivial change. 

The application makes use of two buckets, one for temporary files (that is configured to automatically delete files on a certain interval) and one for data assets like models and datasets that are retained for further use via the application. 

Below are some of the config files used to host the application. Below is the .env file, which provide information related to the particular hosting environmet. MLcode release dictates the version of the ML codebase the application will use in the backend (the application release version is dictated by the application git version used to build the docker container)

Application .env (environmental variables):
```bash
APPNAME: "ftnirs_mlapp"
HOSTIP: "0.0.0.0"
APPPORT: "8050"
PROXYPORT: "443"
DATA_BUCKET: "mlapp_data_bucket"
TMP_BUCKET: "mlapp_tmp_bucket"
MLCODE_RELEASE: "v0.5.5"
```

The docker compose .yml file shows the different containers (Dash application and NGINX web server) that comprise the hosted application. Notice that in this case, docker compose will build the Dash application from a local directory, but you can also point this to a container repository with a prebuilt container if desired. 

Docker compose .yml:
```yaml
services:
  nginx:
    image: nginx:latest
    container_name: nginx_server
    volumes:
      - ./nginx-selfsigned.key:/etc/nginx/cert/nginx-selfsigned.key
      - ./nginx-selfsigned.crt:/etc/nginx/cert/nginx-selfsigned.crt
      - ./template-variables:/etc/nginx/templates/default.conf.template:ro
    ports:
      - ${PROXYPORT}:${PROXYPORT}
      - 80:80
    environment:
      PROXYPORT: ${PROXYPORT}
      APPPORT: ${APPPORT}
      HOSTIP: ${HOSTIP}
      DATA_BUCKET: ${DATA_BUCKET}
      TMP_BUCKET: ${TMP_BUCKET}
      APPNAME: ${APPNAME}
    depends_on:
      - ftnirs_mlapp
    restart: always
  ftnirs_mlapp:
    build:
      context: ../ftnirs-mlapp
      args:
        MLCODE_RELEASE: ${MLCODE_RELEASE}
    container_name: ftnirs-mlapp
    ports: 
      - ${APPPORT}:${APPPORT}
    environment:
      APPNAME: ${APPNAME}
      APPPORT: ${APPPORT}
      HOSTIP: ${HOSTIP}
      DATA_BUCKET: ${DATA_BUCKET}
      TMP_BUCKET: ${TMP_BUCKET}
      MLCODE_RELEASE: ${MLCODE_RELEASE}
    restart: always
```

The NGINX web server also has a .conf file, where you can set routing information for requests to the server, as well as incorporate concepts like SSL and traffic management.   

NGINX.conf:

```conf
server {

  listen ${PROXYPORT} ssl;
  server_name ${HOSTIP};

  ssl_certificate /etc/nginx/cert/nginx-selfsigned.crt;
  ssl_certificate_key /etc/nginx/cert/nginx-selfsigned.key;

  location / {
      root /usr/share/nginx/html;
      index index.html;
  }

  location /ftnirs_mlapp/ {
    proxy_pass http://ftnirs_mlapp:${APPPORT};
  }

  proxy_read_timeout 600;
  proxy_connect_timeout 600;
  proxy_send_timeout 600; 

  client_max_body_size 500M;
}
```

