---
title: Using the Application
---

### The layout

<details>

<summary><font size = "4">Preparing to run models</font></summary>

The app components are ordered to go left right / top to bottom, usually in the order you would modify them in a typical workflow. 

At the very top, we have a few buttons. The first is a link to go to the home page (***NOTE TO SELF, may deprecate if move to one page app***), a link to the documentation (this website), and in the right we have the git versions and buttons with links to the git repos. 

![Navigation](/content/images/app_screenshot_top_buttons.png)

In the top left, we have the data upload / select pane. You can browse through existing datasets in the app or upload further datasets. 

![Data browsing and upload section of the app](/content/images/app_screenshot_data.png)

The datasets will do different things depending on which 'mode' is selected. 

![Mode and approach section of the app](/content/images/app_screenshot_mode.png)

To the right, once one or more datasets are selected, options are available in the Data Columns pane. This allows for selection of which columns will be considered in the model run event. Depending on mode and the dataset(s) selected, you will have different options here and your choices will result in different behaviors (see more details in Data Columns section) 

![Data Columns pane](/content/images/app_screenshot_data_columns.png)

To the right of that, you have options for parameterization. Within this pane, you can choose parameters for signal processing,

![Parameters pane: signal processing](/content/images/app_screenshot_signal_processing.png)

parameters specific to the mode, 

![Parameters pane: mode general](/content/images/app_screenshot_general_train_params.png)

and parameters specific to the training/fine-tuning approach.

![Parameters pane: approach](/content/images/app_screenshot_approach_specific_train_params.png)

Once all of these sections have been considered, you can run the particular model event (will vary depending on mode) with the run button. 

![Run button](/content/images/app_screenshot_run.png)

</details>

<details>

<summary><font size = "4">Running models</font></summary>

The run button will execute the model run event based on the parameters provided in the aforementioned sections.

![Run button](/content/images/app_screenshot_run.png)

While the model is running, there is an intermediate app stage where you will be provided live information. The run button will display as a loading icon during training. Jobs cannot be canceled once started (***NOTE: maybe can change if move to single page app?***). To the left of the run button is logging information for the algorithm process.

![Event logs](/content/images/run_in_progress_logs.png)

To the right is a live display of the training loss and validation loss, used to assess the appropriate degree of model fitting and help select a resonable training time or early stopping criterion. 

![Training loss curve](/content/images/run_in_progress_graph.png)

While model training is ongoing, the numner of concurrent tensorflow jobs in the top bar will increment up by one. This is a global value shared between all users currently training models on the server, and exists to help identify if any errors or poor performance could be related to resource contention.

![Concurrent tensorflow jobs](/content/images/run_in_progress_concurrent.png)

Any errors observed during model training will be displayed in the bottom left, accompanied by a banner error. 

![Errors](/content/images/errors.png)

</details>

<details>

<summary><font size = "4">Model results</font></summary>

If the model run event has finished without errors, you can scroll down to see outputs relevant to the mode selected. 

![Completed run](/content/images/run_complete.png)

In the center section are stats and graphical artifacts demonstrating model behaviors and performance. Depending on whether truth ages were provided, this will show a truth vs prediction scatterplot, a histogram, or both if age data are partially provided. Color values will indicate splits if in training or fine tuning mode, or dataset name if in inference mode.

![Stats and graphical artifacts](/content/images/run_complete_stats.png)

While also mentioned as a live feedback source while running models, the completed training loss curve is available in the top right. Note that all embedded visualizations have built in features such as allowing zooming, hovering points for more precise values, and saving views as an image directly. 

![Training loss artifact](/content/images/run_complete_loss_curve.png)

On the left are the parameters used for the completed run. There are also two fields where you can add some descriptive information on the run: a name and a description, which are metadata which are attached to exported objects once provided. 

![Model run configuration](/content/images/run_complete_configuration.png)

On the right are a few options to export model results. The download results button downloads run outputs (model objects, age predictions, metadata, etc), and the upload trianed model (only available after training or fine-tuning) allows for you to upload a model so it is available for subsequent inference or fine-tuning by any lab users. After upload, it will be immediately available for further workflows using that trained model. 

![Outputs](/content/images/run_complete_outputs.png)

</details>

### Inputing data 

<details>

<summary><font size = "4">Formatting data</font></summary>

First, you will need to identify the data that you'd like to work with. The technique specifically accepts both spectroscopy data along with other per-otolith biological or spatiotemporal contextual data. The codebase and app use a fairy primitive string matching protocol, and they both make assumptions about what certain columns do based on naming, so remember:

**What you name columns matters!**

All of the naming conventions are defined at the codebase level, within the file ["constants"](https://github.com/noaa-afsc/ftnirs-ml-codebase/blob/main/ftnirsml/constants.py) within the ML codebase. Here are some examples of a few of them at the time of writing:

* IDNAME = "id"
* RESPONSENAME = "age"
* WN_STRING_NAME = 'wn**'
* SPLITNAME = "split"

This means that the codebase and application will give columns with these names special treatment, and so when preparing data you will need to be very aware when you need to name your dataset with on of these columns. Here are their definitions:

* id: a unique identifier that should be globally unique to the specific otolith. Keeping the id specific to the identity of the otolith is important, as when many datasets are being used for training there are filters within the app to ensure that incidental duplicate values that lead to dataset contamination and comprimised results are not present  The contents are not as important, this could be a path to an image, an integer key, a hash, etc.

* age: this is the response variable, and while it is not necessary to include in every dataset (such as when this is not available and should be predicted with inference in the app), it is needed for model training or fine-tuning. The app will be flexible if the age column is provided but ages are only available for certain rows- for these datasets, mark missing ages as NA.  

* wn\*\*: this is the syntax that wav numbers should correspond to. The app will assume any column with characters containing 'wn' (***note to self, this is a good back end improvement, should just match to start of string***) is a wav number column. The numbers should directly follow the 'wn' prefix, so a value 8000 would be specified as "wn8000". Decimal values are ok, ie wn8000.0023. The wav numbers can be in ascending or decending order but must be consecutive (cannot have other biological/spatiotemporal columns interspersed between them), and where they are located in the dataframe does not matter. (***note to self: test all these assumptions, reduce if more permissive than remembered***)

* split: OPTIONAL: this is defined if you would like to define custom training splits during training. If you do want to define training splits, you must define this columns with the exact categories "training","validation","test" (you may leave out one or two of these as desired). You can also define this column, and within the app still specify app generated splits as desired.

In addition to these specific values, there is also a registry of 'standard variables', which help combine like data between different groups and also contain information about the type of data which help for how the application treats these data types. Here is an example of how these are defined: 

STANDARD_COLUMN_NAMES = {IDNAME:{'data_type':'unq_text'},
        SPLITNAME:{'data_type':'int'},
        'catch_year' : {'data_type':'int'},
        'catch_month': {'data_type': 'int'},
        'catch_day': {'data_type': 'int'},
        'sex' : {'data_type':'categorical'},
        RESPONSENAME : {'data_type':'numeric'},
        'latitude' : {'data_type':'numeric'},
        'longitude' : {'data_type':'numeric'},
        'region': {'data_type': 'categorical'},
        'fish_length': {'data_type': 'numeric'},
        'fish_weight': {'data_type': 'numeric'},
        'otolith_weight': {'data_type': 'numeric'},
        'gear_depth': {'data_type': 'numeric'},
        'gear_temp': {'data_type': 'numeric'}
        }

In the above, declaring categorical variables will allow for instance the application to automatically apply one-hot encoding to these variables. (***note to self, this is a good back end improvement, define UNITS ***)

![Application interpretation of a dataset based on names of columns in dataset](/content/images/standard_columns.png)

For some examples of properly formatted data, you can refer to sample [datasets](https://github.com/noaa-afsc/ftnirs-ml-codebase/tree/main/tests/Data) in the tests folder of the ML codebase

</details>

<details>

<summary><font size = "4">How and if to split up data</font></summary>

There are two different different philosophies to provide data to the app. One is with larger, expansive datasets meant to cover large portions of your dataset. These should be named and curated for keeping in the app for continuous reuse and shared use. 

Additionaly, you can split up datasets for more specific experiments involving varying along spatiotemporal groupings. While the app will allow you to aggregate data for processing in common, it does not contain features to split up larger datasets into smaller components (it does, however,  allow you to select which columns you'd like to consider in a given model run event). If you are doing a yearly model inference/update workflow, you may choose to load data in different temporally bounded datasets (data_2016.csv, data_2017.csv, etc) which will allow for fine grain control of which to include or leave out. If you are interested in performing experiments over geospatial differences (data_GoA.csv, data_Ber.csv) this is another way you can split up data. 

The app will flag and reject if there are multiple datasets that contain and equal id, to avoid dataset contaminiation issues during training that will comprimise results. It will do this at time of model training, meaning that datasets splitting up the same data in different ways can go into the application simultaneously. With many different possible ways to split up and organize data prior to the app, datasets of this style are best managed for short term use, and will need to get cleared out periodically by site admins to keep the number of total datasets to resonable levels in the dropdowns (***NOTE to self: this is a good QoL feature: temporary datasets.). 

![Application error for non-unique IDs](/content/images/id_nonunique.png)

</details>


<details>

<summary><font size = "4">Loading datasets into the application </font></summary>

To upload data into the app, use the upload button in the top left. 

![Data browsing and upload section of the app](/content/images/app_screenshot_data_zoom.png)

</details>

### Detailed section by section 


<details>

<summary><font size = "4">Select Datasets </font></summary>

This section allows you to select from previously uploaded data and upload new datasets. Any .csv file will be accepted, and the app will attempt to use certain columns based on exact matches to naming convention. Depending on the mode, the purpose of the dataset will be variable. For most downstream operations, datasets are used as a vertically concatenated (think 'rbind' in R) combination of all the selected datasets. Output data will use names and unique hashes of these component datasets for record keeping and more detailed breadowns outside of the app. 

See next section for how these selected datasets are used for the different modes. 

![Data browsing and upload section of the app](/content/images/app_screenshot_data_zoom.png)

</details>

<details>

<summary><font size = "4">Select Mode</font></summary>

ML models that fit the application native format can be directly uploaded. See examples of use of the ML codebase, including the case where you convert an existing model into the compatible application model format. For inputting models generated through the application, simply path to the output .keras.zip file downloaded from a previous training event. 

The datasets you have selected will function differently depending on what mode you select:

* If 'training' is selected, the selected dataset(s) will function as training data. 
* If 'inference' is selected, the selected dataset(s) will function as data upon which inference is applied (output statistics will use existance of 'age' if available, otherwise will report histogram of output ages)
* If 'fine-tuning' is selected, the selected dataset(s) will function as data for which the model is fine tuned on.

<details>

<summary><font size = "4">Training Mode</font></summary>

The training mode is used when a new model is being trained using a combination of user selected training data (indicated in the data selection pan in the top left) and a given techinique ("approach"). The "basic model" approach is a simple and minimal archetecture that is customizable in parameters and is trained in a single training event. The hyperband algorithm is a more sophisticated orchestration technique where model archecture is 'tuned' to optimize performance over a sequence of optimization rounds. 

These approaches are defined in the ML codebase, and must be encorporated into the application code itself. The application code has been designed so addition of additional approaches is straightforward, and in the ["constants"](https://github.com/noaa-afsc/ftnirs-ml-codebase/blob/main/ftnirsml/constants.py) file within the ML codebase a data structure exists (TRAINING APPROACHES) to define particular parameters related to that approach, which allows the app to provide them in a more streamlined way (no code development required). 

![An example of two different available approaches in the training mode](/content/images/app_screenshot_data.png)

</details>

<details>

<summary><font size = "4">Inference mode</font></summary>

The inference mode is used when a previously trained model exists, and you wish to apply it to a different dataset. The inference mode will take this chosen model and attempt, if compatible, to apply it to the user selected training data (indicated in the data selection pan in the top left). Note that the application will aggressively attempt to make datasets compatible, so pay attention to any information provided in the model metadata as well as in the data pane. Datasets run for inference can include the response column. If the response column is provided, R2 scores will be provided, if not, a histogram of ages with be provided, and if ages are partially known, both will be provided. 

</details>

<details>

<summary><font size = "4">Fine-tuning mode</font></summary>

The fine-tuning mode is used when a previously trained model exists, and you wish to retain some information from the previous weights while 'fine-tuning' it on another dataset. This is an operation that takes some nuance, especially considering the shallow depth of most of these models which make it very easy to discard existing weights if number of training epochs and learning rate (***NOTE to self: is this even in here? Add??***) is not chosen carefully. 

</details>

</details>

<details>

<summary><font size = "4">Data Columns</font></summary>

The data columns pane is responsive to both:

* The column names in the selected datsets, and
* The mode selected

The data pane will display the following information: 

1. Wav numbers and spectrography attributes for each different dataset, as well as if they are 'valid' (fit the criteria for wn** columns and are formatted correctly), and equivalent (how many match the same spectroscopy settings)
2. Whether columns match known reserved names (id, split, age), Standard Column names, or other. 
3. The number of datasets where the particular column is included.
4. Any special treatment of that particular column. This can include: 
    * treated as a reserved column name (unselectable- automatically used)
    * one hot encoding was applied (default for standard columns marked as 'categorical' type)

![Information in Data Columns](/content/images/data_columns_lots_o_details.png)


</details>