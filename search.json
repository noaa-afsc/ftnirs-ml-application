[
  {
    "objectID": "content/contApp.html",
    "href": "content/contApp.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Contributing",
      "App features"
    ]
  },
  {
    "objectID": "content/Integrative.html",
    "href": "content/Integrative.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Examples",
      "Integrative"
    ]
  },
  {
    "objectID": "content/FAQ.html",
    "href": "content/FAQ.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "FAQ"
    ]
  },
  {
    "objectID": "content/backScience.html",
    "href": "content/backScience.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Background",
      "The science"
    ]
  },
  {
    "objectID": "content/useApp.html",
    "href": "content/useApp.html",
    "title": "Test page",
    "section": "",
    "text": "First, you will need to identify a suitable dataset. The technique specifically accepts both spectroscopy data along with other per-otolith biological or spatiotemporal contextual data. The codebase and app use a fairy primitive string matching protocol, and they both make assumptions about what certain columns do based on naming, so remember:\nWhat you name columns matters!\nAll of the naming conventions are defined at the codebase level, within the file “constants”. Here are some examples of a few of them at the time of writing:\n\nIDNAME = “id”\nRESPONSENAME = “age”\nWN_STRING_NAME = ’wn**’\nSPLITNAME = “split”\n\nThis means that the codebase and application will give columns with these names special treatment, and so when preparing data you will need to be very aware when you need to name your dataset with on of these columns. Here are their definitions:\n\nid: a unique identifier that should be globally unique to the specific otolith. Keeping the id specific to the identity of the otolith is important, as when many datasets are being used for training there are filters within the app to ensure that incidental duplicate values that lead to dataset contamination and comprimised results are not present (note to self: check: are there?) The contents are not as important, this could be a path to an image, an integer key, a hash, etc.\nage: this is the response variable, and while it is not necessary to include in every dataset (such as when this is not available and should be predicted with inference in the app), it is needed for model training or fine-tuning. The app will be flexible if the age column is provided but ages are only available for certain rows- for these datasets, mark missing ages as NA.\nwn**: this is the syntax that wav numbers should correspond to. The app will assume any column with characters containing ‘wn’ (note to self, this is a good back end improvement, should just match to start of string) is a wav number column. The numbers should directly follow the ‘wn’ prefix, so a value 8000 would be specified as “wn8000”. Decimal values are ok, ie wn8000.0023.\nsplit: OPTIONAL: this is defined if you would like to define custom training splits during training. If you do want to define training splits, you must define this columns with the exact categories “training”,“validation”,“test” (you may leave out one or two of these as desired). You can also define this column, and within the app still specify app generated splits as desired.\n\nIn addition to these specific values, there is also a registry of ‘standard variables’, which help combine like data between different groups and also contain information about the type of data which help for how the application treats these data types. Here is an example of how these are defined:\nSTANDARD_COLUMN_NAMES = {IDNAME:{‘data_type’:‘unq_text’}, SPLITNAME:{‘data_type’:‘int’}, ‘catch_year’ : {‘data_type’:‘int’}, ‘catch_month’: {‘data_type’: ‘int’}, ‘catch_day’: {‘data_type’: ‘int’}, ‘sex’ : {‘data_type’:‘categorical’}, RESPONSENAME : {‘data_type’:‘numeric’}, ‘latitude’ : {‘data_type’:‘numeric’}, ‘longitude’ : {‘data_type’:‘numeric’}, ‘region’: {‘data_type’: ‘categorical’}, ‘fish_length’: {‘data_type’: ‘numeric’}, ‘fish_weight’: {‘data_type’: ‘numeric’}, ‘otolith_weight’: {‘data_type’: ‘numeric’}, ‘gear_depth’: {‘data_type’: ‘numeric’}, ‘gear_temp’: {‘data_type’: ‘numeric’} }\nIn the above, declaring categorical variables will allow for instance the application to automatically apply one-hot encoding to these variables. (note to self, this is a good back end improvement, define UNITS )\n\n\n\n\nSee examples of use of the ML codebase, including the case where you convert an existing model into the compatible model format.",
    "crumbs": [
      "Use",
      "The app"
    ]
  },
  {
    "objectID": "content/useApp.html#getting-started",
    "href": "content/useApp.html#getting-started",
    "title": "Test page",
    "section": "",
    "text": "First, you will need to identify a suitable dataset. The technique specifically accepts both spectroscopy data along with other per-otolith biological or spatiotemporal contextual data. The codebase and app use a fairy primitive string matching protocol, and they both make assumptions about what certain columns do based on naming, so remember:\nWhat you name columns matters!\nAll of the naming conventions are defined at the codebase level, within the file “constants”. Here are some examples of a few of them at the time of writing:\n\nIDNAME = “id”\nRESPONSENAME = “age”\nWN_STRING_NAME = ’wn**’\nSPLITNAME = “split”\n\nThis means that the codebase and application will give columns with these names special treatment, and so when preparing data you will need to be very aware when you need to name your dataset with on of these columns. Here are their definitions:\n\nid: a unique identifier that should be globally unique to the specific otolith. Keeping the id specific to the identity of the otolith is important, as when many datasets are being used for training there are filters within the app to ensure that incidental duplicate values that lead to dataset contamination and comprimised results are not present (note to self: check: are there?) The contents are not as important, this could be a path to an image, an integer key, a hash, etc.\nage: this is the response variable, and while it is not necessary to include in every dataset (such as when this is not available and should be predicted with inference in the app), it is needed for model training or fine-tuning. The app will be flexible if the age column is provided but ages are only available for certain rows- for these datasets, mark missing ages as NA.\nwn**: this is the syntax that wav numbers should correspond to. The app will assume any column with characters containing ‘wn’ (note to self, this is a good back end improvement, should just match to start of string) is a wav number column. The numbers should directly follow the ‘wn’ prefix, so a value 8000 would be specified as “wn8000”. Decimal values are ok, ie wn8000.0023.\nsplit: OPTIONAL: this is defined if you would like to define custom training splits during training. If you do want to define training splits, you must define this columns with the exact categories “training”,“validation”,“test” (you may leave out one or two of these as desired). You can also define this column, and within the app still specify app generated splits as desired.\n\nIn addition to these specific values, there is also a registry of ‘standard variables’, which help combine like data between different groups and also contain information about the type of data which help for how the application treats these data types. Here is an example of how these are defined:\nSTANDARD_COLUMN_NAMES = {IDNAME:{‘data_type’:‘unq_text’}, SPLITNAME:{‘data_type’:‘int’}, ‘catch_year’ : {‘data_type’:‘int’}, ‘catch_month’: {‘data_type’: ‘int’}, ‘catch_day’: {‘data_type’: ‘int’}, ‘sex’ : {‘data_type’:‘categorical’}, RESPONSENAME : {‘data_type’:‘numeric’}, ‘latitude’ : {‘data_type’:‘numeric’}, ‘longitude’ : {‘data_type’:‘numeric’}, ‘region’: {‘data_type’: ‘categorical’}, ‘fish_length’: {‘data_type’: ‘numeric’}, ‘fish_weight’: {‘data_type’: ‘numeric’}, ‘otolith_weight’: {‘data_type’: ‘numeric’}, ‘gear_depth’: {‘data_type’: ‘numeric’}, ‘gear_temp’: {‘data_type’: ‘numeric’} }\nIn the above, declaring categorical variables will allow for instance the application to automatically apply one-hot encoding to these variables. (note to self, this is a good back end improvement, define UNITS )\n\n\n\n\nSee examples of use of the ML codebase, including the case where you convert an existing model into the compatible model format.",
    "crumbs": [
      "Use",
      "The app"
    ]
  },
  {
    "objectID": "content/dataPrep.html",
    "href": "content/dataPrep.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Examples",
      "Data preparation"
    ]
  },
  {
    "objectID": "content/useCodebase.html",
    "href": "content/useCodebase.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Use",
      "The codebase"
    ]
  },
  {
    "objectID": "content/futDirections.html",
    "href": "content/futDirections.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Future Directions"
    ]
  },
  {
    "objectID": "content/exApp.html",
    "href": "content/exApp.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Examples",
      "The app"
    ]
  },
  {
    "objectID": "content/Maintaining.html",
    "href": "content/Maintaining.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Maintaining"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fourier transform near-infrared spectroscopy / machine learning otolith aging application",
    "section": "",
    "text": "The AFSC Age and Growth Program have pioneered machine learning methods to streamline specimen age predictions from otoliths. The approach is a multimodal convolutional neural network, (MM- CNN) as described in Benson et al. (2019), which encorporates two key data modalities: the entire range of wavenumbers of FT-NIR spectra, and corresponding biological and geospatial data.\nDuring development of this technique, scripts was developed in jupyter notebooks and demonstrated excellent performance in multiple species age prediction (cite). To bring these techniques to a wider audience, the code was refactored into functional programming. The core scripts were refactored into a functional programming codebase that encorporated ideas like data standards and the machine learning lifecycle to promote reuse and extention of the method across a broader range of data and machine learning techniques.\nThis codebase was developed in python, and currently supports the following features:\n\nFull machine learning lifecycle:\n\nTraining\nInference\nFine-tuning\n\nModel inference and fine-tuning will use the data given as best possible\n\nAutomatic interpolation of spectra as required\nNew data can be incorporated into fine tuning while preserving existing model weights\n\nBuilt in features for under-the-hood data formatting\n\nautomatic one hot encoding for known categorical variables\nawareness of standard variables to promote combining data where appropriate\n\n\nThe application serves to deliver the codebase core logic in an easy to use interface.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "Fourier transform near-infrared spectroscopy / machine learning otolith aging application",
    "section": "",
    "text": "The AFSC Age and Growth Program have pioneered machine learning methods to streamline specimen age predictions from otoliths. The approach is a multimodal convolutional neural network, (MM- CNN) as described in Benson et al. (2019), which encorporates two key data modalities: the entire range of wavenumbers of FT-NIR spectra, and corresponding biological and geospatial data.\nDuring development of this technique, scripts was developed in jupyter notebooks and demonstrated excellent performance in multiple species age prediction (cite). To bring these techniques to a wider audience, the code was refactored into functional programming. The core scripts were refactored into a functional programming codebase that encorporated ideas like data standards and the machine learning lifecycle to promote reuse and extention of the method across a broader range of data and machine learning techniques.\nThis codebase was developed in python, and currently supports the following features:\n\nFull machine learning lifecycle:\n\nTraining\nInference\nFine-tuning\n\nModel inference and fine-tuning will use the data given as best possible\n\nAutomatic interpolation of spectra as required\nNew data can be incorporated into fine tuning while preserving existing model weights\n\nBuilt in features for under-the-hood data formatting\n\nautomatic one hot encoding for known categorical variables\nawareness of standard variables to promote combining data where appropriate\n\n\nThe application serves to deliver the codebase core logic in an easy to use interface.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "content/contScience.html",
    "href": "content/contScience.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Contributing",
      "Evolving scientific techniques"
    ]
  },
  {
    "objectID": "content/Examples.html",
    "href": "content/Examples.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Examples"
    ]
  },
  {
    "objectID": "content/Contributing.html",
    "href": "content/Contributing.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "content/contDocumentation.html",
    "href": "content/contDocumentation.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Contributing",
      "The documentation"
    ]
  },
  {
    "objectID": "content/backDesign.html",
    "href": "content/backDesign.html",
    "title": "Application design",
    "section": "",
    "text": "To bring the scientific technique into operational state, the AFSC Age & Growth program began work with AFSC OFIS in spring of 2024 to make the work more accessible to non-technical users. The core design of any solution involved:\n\nA python-based ML codebase that contained functions encapsulating the scientific processes\nSoftware that involved delivery of the codebase in an accessible format\n\nAFSC Age & Growth program socialized this idea with other science center researchers and NMFS IT community members, before defining the initial concept.\n\n\n\nOriginal products concept, March 2024",
    "crumbs": [
      "Background",
      "The design"
    ]
  },
  {
    "objectID": "content/backDesign.html#concept",
    "href": "content/backDesign.html#concept",
    "title": "Application design",
    "section": "",
    "text": "To bring the scientific technique into operational state, the AFSC Age & Growth program began work with AFSC OFIS in spring of 2024 to make the work more accessible to non-technical users. The core design of any solution involved:\n\nA python-based ML codebase that contained functions encapsulating the scientific processes\nSoftware that involved delivery of the codebase in an accessible format\n\nAFSC Age & Growth program socialized this idea with other science center researchers and NMFS IT community members, before defining the initial concept.\n\n\n\nOriginal products concept, March 2024",
    "crumbs": [
      "Background",
      "The design"
    ]
  },
  {
    "objectID": "content/backDesign.html#software-stack",
    "href": "content/backDesign.html#software-stack",
    "title": "Application design",
    "section": "Software stack",
    "text": "Software stack\n\nOperating system:\nBoth codebases are 100% python based, and are cross OS compatible. The application and the ML codebase can, and are, run on both Windows and Linux hosts.\n\n\nPython:\nPython is popular programming language with many useful packages for the underlying processes:\n\nML codebase core packages:\n\nWorking with data tables (pandas)\nMachine learning (tensorflow + keras)\n\nApplication core packages:\n\nApplication front-end (Dash)\nData and machine learning backend (ML codebase)\n\n\n\n\nPersistent data:\nThe application itself is stateless, meaning a new hosted copy of the application will perform all needed functions. The application uses Google Cloud Storage on the backend to store persistent data. The application authenticates to Google credentials to communicate with the storage location, and so requires a Google Cloud Project, and the application needs to have access to personal credentials (when running for individual use) or a service account (when running for shared use).\nWe are currently hosting the application on GCP with an Ubuntu server, using NGINX for publication to an NMFS internal https port.",
    "crumbs": [
      "Background",
      "The design"
    ]
  },
  {
    "objectID": "content/exCodebase.html",
    "href": "content/exCodebase.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Examples",
      "The codebase"
    ]
  },
  {
    "objectID": "content/Background.html",
    "href": "content/Background.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "content/maintApp.html",
    "href": "content/maintApp.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Maintaining",
      "App data and models"
    ]
  },
  {
    "objectID": "content/selfHost.html",
    "href": "content/selfHost.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Maintaining",
      "Self-hosting"
    ]
  },
  {
    "objectID": "content/Use.html",
    "href": "content/Use.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Use"
    ]
  }
]