[
  {
    "objectID": "content/contApp.html",
    "href": "content/contApp.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Contributing",
      "App features"
    ]
  },
  {
    "objectID": "content/Integrative.html",
    "href": "content/Integrative.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Examples",
      "Integrative"
    ]
  },
  {
    "objectID": "content/FAQ.html",
    "href": "content/FAQ.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "FAQ"
    ]
  },
  {
    "objectID": "content/backScience.html",
    "href": "content/backScience.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Background",
      "The science"
    ]
  },
  {
    "objectID": "content/useApp.html",
    "href": "content/useApp.html",
    "title": "Using the Application",
    "section": "",
    "text": "The layout\n\n\nPreparing to run models\n\nThe app components are ordered to go left right / top to bottom, usually in the order you would modify them in a typical workflow.\nAt the very top, we have a few buttons. The first is a link to go to the home page (NOTE TO SELF, may deprecate if move to one page app), a link to the documentation (this website), and in the right we have the git versions and buttons with links to the git repos.\n\n\n\nNavigation\n\n\nIn the top left, we have the data upload / select pane. You can browse through existing datasets in the app or upload further datasets.\n\n\n\nData browsing and upload section of the app\n\n\nThe datasets will do different things depending on which ‘mode’ is selected.\n\n\n\nMode and approach section of the app\n\n\nTo the right, once one or more datasets are selected, options are available in the Data Columns pane. This allows for selection of which columns will be considered in the model run event. Depending on mode and the dataset(s) selected, you will have different options here and your choices will result in different behaviors (see more details in Data Columns section)\n\n\n\nData Columns pane\n\n\nTo the right of that, you have options for parameterization. Within this pane, you can choose parameters for signal processing,\n\n\n\nParameters pane: signal processing\n\n\nparameters specific to the mode,\n\n\n\nParameters pane: mode general\n\n\nand parameters specific to the training/fine-tuning approach.\n\n\n\nParameters pane: approach\n\n\nOnce all of these sections have been considered, you can run the particular model event (will vary depending on mode) with the run button.\n\n\n\nRun button\n\n\n\n\n\nRunning models\n\nThe run button will execute the model run event based on the parameters provided in the aforementioned sections.\n\n\n\nRun button\n\n\nWhile the model is running, there is an intermediate app stage where you will be provided live information. The run button will display as a loading icon during training. Jobs cannot be canceled once started (NOTE: maybe can change if move to single page app?). To the left of the run button is logging information for the algorithm process.\n\n\n\nEvent logs\n\n\nTo the right is a live display of the training loss and validation loss, used to assess the appropriate degree of model fitting and help select a resonable training time or early stopping criterion.\n\n\n\nTraining loss curve\n\n\nWhile model training is ongoing, the numner of concurrent tensorflow jobs in the top bar will increment up by one. This is a global value shared between all users currently training models on the server, and exists to help identify if any errors or poor performance could be related to resource contention.\n\n\n\nConcurrent tensorflow jobs\n\n\nAny errors observed during model training will be displayed in the bottom left, accompanied by a banner error.\n\n\n\nErrors\n\n\n\n\n\nModel results\n\nIf the model run event has finished without errors, you can scroll down to see outputs relevant to the mode selected.\n\n\n\nCompleted run\n\n\nIn the center section are stats and graphical artifacts demonstrating model behaviors and performance. Depending on whether truth ages were provided, this will show a truth vs prediction scatterplot, a histogram, or both if age data are partially provided. Color values will indicate splits if in training or fine tuning mode, or dataset name if in inference mode.\n\n\n\nStats and graphical artifacts\n\n\nWhile also mentioned as a live feedback source while running models, the completed training loss curve is available in the top right. Note that all embedded visualizations have built in features such as allowing zooming, hovering points for more precise values, and saving views as an image directly.\n\n\n\nTraining loss artifact\n\n\nOn the left are the parameters used for the completed run. There are also two fields where you can add some descriptive information on the run: a name and a description, which are metadata which are attached to exported objects once provided.\n\n\n\nModel run configuration\n\n\nOn the right are a few options to export model results. The download results button downloads run outputs (model objects, age predictions, metadata, etc), and the upload trianed model (only available after training or fine-tuning) allows for you to upload a model so it is available for subsequent inference or fine-tuning by any lab users. After upload, it will be immediately available for further workflows using that trained model.\n\n\n\nOutputs\n\n\n\n\n\nInputing data\n\n\nFormatting data\n\nFirst, you will need to identify the data that you’d like to work with. The technique specifically accepts both spectroscopy data along with other per-otolith biological or spatiotemporal contextual data. The codebase and app use a fairy primitive string matching protocol, and they both make assumptions about what certain columns do based on naming, so remember:\nWhat you name columns matters!\nAll of the naming conventions are defined at the codebase level, within the file “constants” within the ML codebase. Here are some examples of a few of them at the time of writing:\n\nIDNAME = “id”\nRESPONSENAME = “age”\nWN_STRING_NAME = ’wn**’\nSPLITNAME = “split”\n\nThis means that the codebase and application will give columns with these names special treatment, and so when preparing data you will need to be very aware when you need to name your dataset with on of these columns. Here are their definitions:\n\nid: a unique identifier that should be globally unique to the specific otolith. Keeping the id specific to the identity of the otolith is important, as when many datasets are being used for training there are filters within the app to ensure that incidental duplicate values that lead to dataset contamination and comprimised results are not present The contents are not as important, this could be a path to an image, an integer key, a hash, etc.\nage: this is the response variable, and while it is not necessary to include in every dataset (such as when this is not available and should be predicted with inference in the app), it is needed for model training or fine-tuning. The app will be flexible if the age column is provided but ages are only available for certain rows- for these datasets, mark missing ages as NA.\nwn**: this is the syntax that wav numbers should correspond to. The app will assume any column with characters containing ‘wn’ (note to self, this is a good back end improvement, should just match to start of string) is a wav number column. The numbers should directly follow the ‘wn’ prefix, so a value 8000 would be specified as “wn8000”. Decimal values are ok, ie wn8000.0023. The wav numbers can be in ascending or decending order but must be consecutive (cannot have other biological/spatiotemporal columns interspersed between them), and where they are located in the dataframe does not matter. (note to self: test all these assumptions, reduce if more permissive than remembered)\nsplit: OPTIONAL: this is defined if you would like to define custom training splits during training. If you do want to define training splits, you must define this columns with the exact categories “training”,“validation”,“test” (you may leave out one or two of these as desired). You can also define this column, and within the app still specify app generated splits as desired.\n\nIn addition to these specific values, there is also a registry of ‘standard variables’, which help combine like data between different groups and also contain information about the type of data which help for how the application treats these data types. Here is an example of how these are defined:\nSTANDARD_COLUMN_NAMES = {IDNAME:{‘data_type’:‘unq_text’}, SPLITNAME:{‘data_type’:‘int’}, ‘catch_year’ : {‘data_type’:‘int’}, ‘catch_month’: {‘data_type’: ‘int’}, ‘catch_day’: {‘data_type’: ‘int’}, ‘sex’ : {‘data_type’:‘categorical’}, RESPONSENAME : {‘data_type’:‘numeric’}, ‘latitude’ : {‘data_type’:‘numeric’}, ‘longitude’ : {‘data_type’:‘numeric’}, ‘region’: {‘data_type’: ‘categorical’}, ‘fish_length’: {‘data_type’: ‘numeric’}, ‘fish_weight’: {‘data_type’: ‘numeric’}, ‘otolith_weight’: {‘data_type’: ‘numeric’}, ‘gear_depth’: {‘data_type’: ‘numeric’}, ‘gear_temp’: {‘data_type’: ‘numeric’} }\nIn the above, declaring categorical variables will allow for instance the application to automatically apply one-hot encoding to these variables. (note to self, this is a good back end improvement, define UNITS in this structure)\n***NOTE: there is also a sneaky/unexpected behavior to watch out for. The codebase relies on a simple string match to determine whether a column represents a ‘one-hot-encoded’ column once the one hot encoded categorical column has been expanded into multiple columns. This is essential a reserved sequence of characters, that if they conflict with column names may result in adverse behaviors. The reserved sequence of characters is defined in the [“constants”] file (https://github.com/noaa-afsc/ftnirs-ml-codebase/blob/main/ftnirsml/constants.py) within the ML codebase, as ONE_HOT_FLAG. This should be kept static while data are in training cycles to allow for consistency of how these one-hot-encoded columns are treated.\n\n\n\nApplication interpretation of a dataset based on names of columns in dataset\n\n\nFor some examples of properly formatted data, you can refer to sample datasets in the tests folder of the ML codebase\n\n\n\nHow and if to split up data\n\nThere are two different different philosophies to provide data to the app. One is with larger, expansive datasets meant to cover large portions of your dataset. These should be named and curated for keeping in the app for continuous reuse and shared use.\nAdditionaly, you can split up datasets for more specific experiments involving varying along spatiotemporal groupings. While the app will allow you to aggregate data for processing in common, it does not contain features to split up larger datasets into smaller components (it does, however, allow you to select which columns you’d like to consider in a given model run event). If you are doing a yearly model inference/update workflow, you may choose to load data in different temporally bounded datasets (data_2016.csv, data_2017.csv, etc) which will allow for fine grain control of which to include or leave out. If you are interested in performing experiments over geospatial differences (data_GoA.csv, data_Ber.csv) this is another way you can split up data.\nThe app will flag and reject if there are multiple datasets that contain and equal id, to avoid dataset contaminiation issues during training that will comprimise results. It will do this at time of model training, meaning that datasets splitting up the same data in different ways can go into the application simultaneously. With many different possible ways to split up and organize data prior to the app, datasets of this style are best managed for short term use, and will need to get cleared out periodically by site admins to keep the number of total datasets to resonable levels in the dropdowns (***NOTE to self: this is a good QoL feature: temporary datasets.).\n\n\n\nApplication error for non-unique IDs\n\n\n\n\n\nLoading datasets into the application \n\nTo upload data into the app, use the upload button in the top left.\n\n\n\nData browsing and upload section of the app\n\n\n\n\n\nDetailed section by section\n\n\nSelect Datasets \n\nThis section allows you to select from previously uploaded data and upload new datasets. Any .csv file will be accepted, and the app will attempt to use certain columns based on exact matches to naming convention. Depending on the mode, the purpose of the dataset will be variable. For most downstream operations, datasets are used as a vertically concatenated (think ‘rbind’ in R) combination of all the selected datasets. Output data will use names and unique hashes of these component datasets for record keeping and more detailed breadowns outside of the app.\nSee next section for how these selected datasets are used for the different modes.\n\n\n\nData browsing and upload section of the app\n\n\n\n\n\nSelect Mode\n\nML models that fit the application native format can be directly uploaded. See examples of use of the ML codebase, including the case where you convert an existing model into the compatible application model format. For inputting models generated through the application, simply path to the output .keras.zip file downloaded from a previous training event.\nThe datasets you have selected will function differently depending on what mode you select:\n\nIf ‘training’ is selected, the selected dataset(s) will function as training data.\nIf ‘inference’ is selected, the selected dataset(s) will function as data upon which inference is applied (output statistics will use existance of ‘age’ if available, otherwise will report histogram of output ages)\nIf ‘fine-tuning’ is selected, the selected dataset(s) will function as data for which the model is fine tuned on.\n\n\n\nTraining Mode\n\nThe training mode is used when a new model is being trained using a combination of user selected training data (indicated in the data selection pan in the top left) and a given techinique (“approach”). The “basic model” approach is a simple and minimal archetecture that is customizable in parameters and is trained in a single training event. The hyperband algorithm is a more sophisticated orchestration technique where model archecture is ‘tuned’ to optimize performance over a sequence of optimization rounds.\nThese approaches are defined in the ML codebase, and must be encorporated into the application code itself. The application code has been designed so addition of additional approaches is straightforward.\n\n\n\nAn example of two different available approaches in the training mode\n\n\n\n\n\nInference mode\n\nThe inference mode is used when a previously trained model exists, and you wish to apply it to a different dataset. The inference mode will take this chosen model and attempt, if compatible, to apply it to the user selected training data (indicated in the data selection pan in the top left). Note that the application will aggressively attempt to make datasets compatible, so pay attention to any information provided in the model metadata as well as in the data pane. Datasets run for inference can include the response column. If the response column is provided, R2 scores will be provided, if not, a histogram of ages with be provided, and if ages are partially known, both will be provided.\nWhen a trained model is selected, the app will display any available metadata. This can help assess compatibility with different datasets. This also includes metadata on any description given to the model (good practice is to discuss the training data used), as well as other information relevant for fine-tuning.\n\n\n\nModel metadata, inference\n\n\n\n\n\nFine-tuning mode\n\nThe fine-tuning mode is used when a previously trained model exists, and you wish to retain some information from the previous weights while ‘fine-tuning’ it on another dataset. This is an operation that takes some nuance, especially considering the shallow depth of most of these models which make it very easy to discard existing weights if number of training epochs and learning rate (NOTE to self: is this even in here? Add??) is not chosen carefully.\nThe model metadata contains information relevant for fine-tuning, including the max_bio_columns, which is relevant for fine-tuning as it represents the total amount of non-wavenumber columns supported by the model archetecture for further fine-tuning. If the number of non-wavenumber columns exceeds this value, further retraining will not be possible.\n\n\n\n\nData Columns\n\nThe data columns pane is responsive to both:\n\nThe column names in the selected datsets, and\nThe mode selected\n\nThe data pane will display the following information:\n\nWhether a given data column is selectable (optional) for the particular mode. Columns that are not selectable will generally say why.\n\nWav numbers and spectrography attributes for each different dataset, as well as if they are ‘valid’ (fit the criteria for wn** columns and are formatted correctly), and equivalent (how many match the same spectroscopy settings)\nWhether columns match known reserved names (id, split, age), Standard Column names, or other.\nThe number of datasets where the particular column is included.\nAny special treatment of that particular column. This can include:\n\ntreated as a reserved column name (unselectable- automatically used)\none hot encoding was applied (default for standard columns marked as ‘categorical’ type)\ninformation on whether or not columns present in a selected model archetecture are present in provided datasets.\n\n\n\n\n\nInformation in Data Columns\n\n\n\n\nTraining Mode\n\nThe data pane will provide the above information, and should be fairly self explanatory. Data that are unselected will not be used to train the model. You may choose to unselect or retain partial matches between the different selected datasetes. Sometimes, it may help to remove partial matches for more consistent model performance between different datasets, whereas conversely it may help optimize for performance to leave certain partially available columns in.\n\n\n\nExamples of information feedback from Data Pane from selections; training mode\n\n\n\n\n\nInference Mode\n\nWhen a trained model is selected, and there are differences between the data columns used to train the model and the data columns available in the selected dataset, the application will unselect the data columns as an option without options to reselect, and state why. This could be either that:\n\nthe column is included in pretrained model (but absent from dataset)\nthe column is not used in pretrained model (but present in dataset)\n\nFor data columns that are present in both the dataset(s) and pretrained model, it will be optional to include them. When the data are fully available, you would usually elect to include available data that match the trained model. In cases where the selected datasets partially match what the model has available, you may prefer to remove a partially available column to allow for more equal treament by the model among the datasets. You can also choose to unselect certain data for experimental reasons (for example, to simulate the effect of a potentially absent data column in the future).\n\n\n\nExamples of information feedback from Data Pane from selections; inference mode\n\n\n\n\n\nFine-tuning Mode\n\nFor the fine-tuning mode, the Data Column section is a mix of the behaviors of training and inference. The model archetectures on first training are hardcoded (NOTE: for now) to provide a maximum of 100 biological/geospatial/temporal features for future trainings, allowing the same weights to be preserved while being flexible to dynamic data availability. This means that datasets that are now available for retraining will be possible to include even if not present in the original model. The Data Column pane will flag standard columns which, like in inference, are either present on the model archetecture and absent in the data (included in pretrained model), but will also provide this message when columns in the dataset are included in the pretrained model to help distinguish and clarify decisions when it comes to choosing to include/exlude new data or data already present in the model during retraining.\n\n\n\nExamples of information feedback from Data Pane from selections; fine-tuning mode\n\n\n\nBoth inference and fine-tuning will flag if a column was NULLIFIED in the last training event, meaning, the biological/geospatial/temporal data was named and included some point in the model training, but in the latest round was given a dummy value to deprioritize it for retraining. For inference, a column currently nullified shouldn’t be used except for experimental reasons. NULLIFIED columns can be added back in as desired for fine-tuning, but care should be taken when reintroducing it as performance impacts can be unpredictable.\n\n\n\nData Pane showing columns that were nullifed in last retraining\n\n\n\n\n\nSelect Parameters\n\nThe Select Parameters section allows you to define relevant parameters for the model run event. This is broken down into three component sections: signal processing parameters, mode parameters, and approach parameters. Some number of these are applicable for each mode, and the way each component section is interfaced with changes depending on the selected mode.\n\n\nSignal processing parameters\n\nThis section is where signal processing algorithms are selected. These can only be changed in a new training event.\n\n\n\nSignal processing options during training\n\n\nInterpolation can be set to custom bounds to promote interoperability in future use, otherwise it will default to the minimum and maximum values present in the training dataset.\n\n\n\nSignal processing options during training; custom interpoloation\n\n\nDuring inference or retraining, this section shows the preset parameters which will be applied to the model run but are unchangeable. If the wav numbers have different attributes (start, end, step), 1d interpolation will be applied to attempt to make the signal compatible.\n\n\n\nFixed signal processing choices for inference and retraining\n\n\n\n\n\nMode parameters\n\nMode parameters are generic to a specific mode: training, inference, or fine-tuning. Training options include a seed (to help ensure reproduceable initial random weights), custom defined splits, number of epochs, and early stopping parameters (note that manual early stopping is not currently supported).\nCurrently, there are no parameters specific to inference.\n\n\n\nParameters for training/fine-tuning modes\n\n\n\n\n\nApproach parameters\n\nMode parameters are generic to a specific training approach. Approaches are meant to expand and vary to cover different resonable scientific approaches to model training. The parameters vary depending on the approach, and when developing new approaches, you can define these using a special data structure in the ML codebase to avoid having to develop the front end code to encorporate them (TRAINING APPROACHES variable in the “constants” file within the ML codebase)\n\n\n\nParameters for training/fine tuning approaches",
    "crumbs": [
      "Use",
      "The app"
    ]
  },
  {
    "objectID": "content/dataPrep.html",
    "href": "content/dataPrep.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Examples",
      "Data preparation"
    ]
  },
  {
    "objectID": "content/useCodebase.html",
    "href": "content/useCodebase.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Use",
      "The codebase"
    ]
  },
  {
    "objectID": "content/futDirections.html",
    "href": "content/futDirections.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Future Directions"
    ]
  },
  {
    "objectID": "content/exApp.html",
    "href": "content/exApp.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Examples",
      "The app"
    ]
  },
  {
    "objectID": "content/Maintaining.html",
    "href": "content/Maintaining.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Maintaining"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fourier transform near-infrared spectroscopy / machine learning otolith aging application",
    "section": "",
    "text": "The AFSC Age and Growth Program have pioneered machine learning methods to streamline specimen age predictions from otoliths. The approach is a multimodal convolutional neural network, (MM- CNN) as described in Benson et al. (2019), which encorporates two key data modalities: the entire range of wavenumbers of FT-NIR spectra, and corresponding biological and geospatial data.\nDuring development of this technique, scripts was developed in jupyter notebooks and demonstrated excellent performance in multiple species age prediction (cite). To bring these techniques to a wider audience, the code was refactored into functional programming. The core scripts were refactored into a functional programming codebase that encorporated ideas like data standards and the machine learning lifecycle to promote reuse and extention of the method across a broader range of data and machine learning techniques.\nThis codebase was developed in python, and currently supports the following features:\n\nFull machine learning lifecycle:\n\nTraining\nInference\nFine-tuning\n\nModel inference and fine-tuning will use the data given as best possible\n\nAutomatic interpolation of spectra as required\nNew data can be incorporated into fine tuning while preserving existing model weights\n\nBuilt in features for under-the-hood data formatting\n\nautomatic one hot encoding for known categorical variables\nawareness of standard variables to promote combining data where appropriate\n\n\nThe application serves to deliver the codebase core logic in an easy to use interface.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "Fourier transform near-infrared spectroscopy / machine learning otolith aging application",
    "section": "",
    "text": "The AFSC Age and Growth Program have pioneered machine learning methods to streamline specimen age predictions from otoliths. The approach is a multimodal convolutional neural network, (MM- CNN) as described in Benson et al. (2019), which encorporates two key data modalities: the entire range of wavenumbers of FT-NIR spectra, and corresponding biological and geospatial data.\nDuring development of this technique, scripts was developed in jupyter notebooks and demonstrated excellent performance in multiple species age prediction (cite). To bring these techniques to a wider audience, the code was refactored into functional programming. The core scripts were refactored into a functional programming codebase that encorporated ideas like data standards and the machine learning lifecycle to promote reuse and extention of the method across a broader range of data and machine learning techniques.\nThis codebase was developed in python, and currently supports the following features:\n\nFull machine learning lifecycle:\n\nTraining\nInference\nFine-tuning\n\nModel inference and fine-tuning will use the data given as best possible\n\nAutomatic interpolation of spectra as required\nNew data can be incorporated into fine tuning while preserving existing model weights\n\nBuilt in features for under-the-hood data formatting\n\nautomatic one hot encoding for known categorical variables\nawareness of standard variables to promote combining data where appropriate\n\n\nThe application serves to deliver the codebase core logic in an easy to use interface.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "content/contScience.html",
    "href": "content/contScience.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Contributing",
      "Evolving scientific techniques"
    ]
  },
  {
    "objectID": "content/Examples.html",
    "href": "content/Examples.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Examples"
    ]
  },
  {
    "objectID": "content/Contributing.html",
    "href": "content/Contributing.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "content/contDocumentation.html",
    "href": "content/contDocumentation.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Contributing",
      "The documentation"
    ]
  },
  {
    "objectID": "content/backDesign.html",
    "href": "content/backDesign.html",
    "title": "Application design",
    "section": "",
    "text": "To bring the scientific technique into operational state, the AFSC Age & Growth program began work with AFSC OFIS in spring of 2024 to make the work more accessible to non-technical users. The core design of any solution involved:\n\nA python-based ML codebase that contained functions encapsulating the scientific processes\nSoftware that involved delivery of the codebase in an accessible format\n\nAFSC Age & Growth program socialized this idea with other science center researchers and NMFS IT community members, before defining the initial concept.\n\n\n\nOriginal products concept, March 2024",
    "crumbs": [
      "Background",
      "The design"
    ]
  },
  {
    "objectID": "content/backDesign.html#concept",
    "href": "content/backDesign.html#concept",
    "title": "Application design",
    "section": "",
    "text": "To bring the scientific technique into operational state, the AFSC Age & Growth program began work with AFSC OFIS in spring of 2024 to make the work more accessible to non-technical users. The core design of any solution involved:\n\nA python-based ML codebase that contained functions encapsulating the scientific processes\nSoftware that involved delivery of the codebase in an accessible format\n\nAFSC Age & Growth program socialized this idea with other science center researchers and NMFS IT community members, before defining the initial concept.\n\n\n\nOriginal products concept, March 2024",
    "crumbs": [
      "Background",
      "The design"
    ]
  },
  {
    "objectID": "content/backDesign.html#software-stack",
    "href": "content/backDesign.html#software-stack",
    "title": "Application design",
    "section": "Software stack",
    "text": "Software stack\n\nOperating system:\nBoth codebases are 100% python based, and are cross OS compatible. The application and the ML codebase can, and are, run on both Windows and Linux hosts.\n\n\nPython:\nPython is popular programming language with many useful packages for the underlying processes:\n\nML codebase core packages:\n\nWorking with data tables (pandas)\nMachine learning (tensorflow + keras)\n\nApplication core packages:\n\nApplication front-end (Dash)\nData and machine learning backend (ML codebase)\n\n\n\n\nPersistent data:\nThe application itself is stateless, meaning a new hosted copy of the application will perform all needed functions. The application uses Google Cloud Storage on the backend to store persistent data. The application authenticates to Google credentials to communicate with the storage location, and so requires a Google Cloud Project, and the application needs to have access to personal credentials (when running for individual use) or a service account (when running for shared use).\nAFSC OFIS/Age and Growth are currently hosting site assets (the server, and the backend cloud storage), on a NOAA GCP organization cloud project belonging to Age and Growth and adminstrated by AFSC OFIS. A hardened (meaning, AFSC secure) Ubuntu Pro image hosts the application in a docker compose stack, and uses NGINX for publication to an NMFS internal https port.",
    "crumbs": [
      "Background",
      "The design"
    ]
  },
  {
    "objectID": "content/exCodebase.html",
    "href": "content/exCodebase.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Examples",
      "The codebase"
    ]
  },
  {
    "objectID": "content/Background.html",
    "href": "content/Background.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "content/maintApp.html",
    "href": "content/maintApp.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Maintaining",
      "App data and models"
    ]
  },
  {
    "objectID": "content/selfHost.html",
    "href": "content/selfHost.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Maintaining",
      "Self-hosting"
    ]
  },
  {
    "objectID": "content/Use.html",
    "href": "content/Use.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Use"
    ]
  }
]