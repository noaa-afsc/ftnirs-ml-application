[
  {
    "objectID": "content/contApp.html",
    "href": "content/contApp.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Contributing",
      "App features"
    ]
  },
  {
    "objectID": "content/Integrative.html",
    "href": "content/Integrative.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Examples",
      "Integrative"
    ]
  },
  {
    "objectID": "content/FAQ.html",
    "href": "content/FAQ.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "FAQ"
    ]
  },
  {
    "objectID": "content/backScience.html",
    "href": "content/backScience.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Background",
      "The science"
    ]
  },
  {
    "objectID": "content/useApp.html",
    "href": "content/useApp.html",
    "title": "Using the Application",
    "section": "",
    "text": "The layout\n\n\nPreparing to run models\n\nThe app components are ordered to go left right / top to bottom, usually in the order you would modify them in a typical workflow.\nAt the very top, we have a few buttons. The first is a link to go to the home page (NOTE TO SELF, may deprecate if move to one page app), a link to the documentation (this website), and in the right we have the git versions and buttons with links to the git repos.\n\n\n\nNavigation\n\n\nIn the top left, we have the data upload / select pane.\n\n\n\nData browsing and upload section of the app\n\n\nYou can browse through existing datasets in the app or upload further datasets. The datasets will do different things depending on which ‘mode’ is selected (see more details in Mode, Models, and Approaches section).\n\n\n\nMode and approach section of the app\n\n\nTo the right, once one or more datasets are selected, options are available in the Data Columns pane. This allows for selection of which columns will be considered in the model run event. Depending on mode and the dataset(s) selected, you will have different options here and your choices will result in different behaviors (see more details in Data Columns section)\n\n\n\nData Columns pane\n\n\nTo the right of that, you have options for parameterization. Within this pane, you can choose parameters for signal processing,\n\n\n\nParameters pane: signal processing\n\n\nparameters specific to the mode,\n\n\n\nParameters pane: mode general\n\n\nand parameters specific to the training/fine-tuning approach.\n\n\n\nParameters pane: approach\n\n\nOnce all of these sections have been considered, you can run the particular model event (will vary depending on mode) with the run button.\n\n\n\nRun button\n\n\n\n\nRunning models\n\nThe run button will execute the model run event based on the parameters provided in the aforementioned sections.\n\n\n\nRun button\n\n\nWhile the model is running, there is an intermediate app stage where you will be provided live information. The run button will display as a loading icon during training. Jobs cannot be canceled once started (NOTE: maybe can change if move to single page app?) To the left of the run button is logging information for the algorithm process\n\n\n\nRun button\n\n\nTo the right, is a live display of the training loss and validation loss, commonly used to assess the training time to ensure an appropriate degree of model fitting.\n\n\n\nTraining graph\n\n\nWhile model training is ongoing, you will also reserve the # of concurrent tensorflow jobs in the top bar increment up. This is a global value shared between all users currently training models on the server, and exists to help identify if any errors or poor performance could be related to resource contention\n\n\n\nTraining graph\n\n\nAny errors observed during\n\n\n\nInputing data\n\n\nFormatting data\n\nFirst, you will need to identify the data that you’d like to work with. The technique specifically accepts both spectroscopy data along with other per-otolith biological or spatiotemporal contextual data. The codebase and app use a fairy primitive string matching protocol, and they both make assumptions about what certain columns do based on naming, so remember:\nWhat you name columns matters!\nAll of the naming conventions are defined at the codebase level, within the file “constants” within the ML codebase. Here are some examples of a few of them at the time of writing:\n\nIDNAME = “id”\nRESPONSENAME = “age”\nWN_STRING_NAME = ’wn**’\nSPLITNAME = “split”\n\nThis means that the codebase and application will give columns with these names special treatment, and so when preparing data you will need to be very aware when you need to name your dataset with on of these columns. Here are their definitions:\n\nid: a unique identifier that should be globally unique to the specific otolith. Keeping the id specific to the identity of the otolith is important, as when many datasets are being used for training there are filters within the app to ensure that incidental duplicate values that lead to dataset contamination and comprimised results are not present The contents are not as important, this could be a path to an image, an integer key, a hash, etc.\nage: this is the response variable, and while it is not necessary to include in every dataset (such as when this is not available and should be predicted with inference in the app), it is needed for model training or fine-tuning. The app will be flexible if the age column is provided but ages are only available for certain rows- for these datasets, mark missing ages as NA.\nwn**: this is the syntax that wav numbers should correspond to. The app will assume any column with characters containing ‘wn’ (note to self, this is a good back end improvement, should just match to start of string) is a wav number column. The numbers should directly follow the ‘wn’ prefix, so a value 8000 would be specified as “wn8000”. Decimal values are ok, ie wn8000.0023. The wav numbers can be in ascending or decending order but must be consecutive (cannot have other biological/spatiotemporal columns interspersed between them), and where they are located in the dataframe does not matter. (note to self: test all these assumptions, reduce if more permissive than remembered)\nsplit: OPTIONAL: this is defined if you would like to define custom training splits during training. If you do want to define training splits, you must define this columns with the exact categories “training”,“validation”,“test” (you may leave out one or two of these as desired). You can also define this column, and within the app still specify app generated splits as desired.\n\nIn addition to these specific values, there is also a registry of ‘standard variables’, which help combine like data between different groups and also contain information about the type of data which help for how the application treats these data types. Here is an example of how these are defined:\nSTANDARD_COLUMN_NAMES = {IDNAME:{‘data_type’:‘unq_text’}, SPLITNAME:{‘data_type’:‘int’}, ‘catch_year’ : {‘data_type’:‘int’}, ‘catch_month’: {‘data_type’: ‘int’}, ‘catch_day’: {‘data_type’: ‘int’}, ‘sex’ : {‘data_type’:‘categorical’}, RESPONSENAME : {‘data_type’:‘numeric’}, ‘latitude’ : {‘data_type’:‘numeric’}, ‘longitude’ : {‘data_type’:‘numeric’}, ‘region’: {‘data_type’: ‘categorical’}, ‘fish_length’: {‘data_type’: ‘numeric’}, ‘fish_weight’: {‘data_type’: ‘numeric’}, ‘otolith_weight’: {‘data_type’: ‘numeric’}, ‘gear_depth’: {‘data_type’: ‘numeric’}, ‘gear_temp’: {‘data_type’: ‘numeric’} }\nIn the above, declaring categorical variables will allow for instance the application to automatically apply one-hot encoding to these variables. (note to self, this is a good back end improvement, define UNITS )\n\n\n\nApplication interpretation of a dataset based on names of columns in dataset\n\n\nFor some examples of properly formatted data, you can refer to sample datasets in the tests folder of the ML codebase\n\n\n\nChunking data\n\nThere are two ways to consolidate data in the app. One is with larger, expansive datasets meant to cover large portions of your dataset. These should be named and curated for keeping in the app for continuous reuse and shared use.\nAdditionaly, you can split up datasets for more specific experiments involving varying along spatiotemporal groupings. While the app will allow you to aggregate data for processing in common, it does not contain features to split up larger datasets into smaller components (it does, however, allow you to select which columns you’d like to consider in a given model run event). If you are doing a yearly model inference/update workflow, you may choose to load data in different temporally bounded datasets (data_2016.csv, data_2017.csv, etc) which will allow for fine grain control of which to include or leave out. If you are interested in performing experiments over geospatial differences (data_GoA.csv, data_Ber.csv) this is another way you can split up data.\nThe app will flag and reject if there are multiple datasets that contain and equal id, to avoid dataset contaminiation issues during training that will comprimise results. It will do this at time of model training, meaning that datasets splitting up the same data in different ways can go into the application simultaneously. With many different possible ways to split up and organize data prior to the app, datasets of this style are best managed for short term use, and will need to get cleared out periodically by site admins to keep the number of total datasets to resonable levels in the dropdowns.\n\n\n\nApplication error for non-unique IDs\n\n\n\n\n\nLoading datasets into the application \n\nTo upload data into the app, use the upload button in the top left.\n\n\n\nData browsing and upload section of the app\n\n\n\n\n\nMode, models, and approaches\nThe ML codebase contains better features for inputing models not generated through the application. See examples of use of the ML codebase, including the case where you convert an existing model into the compatible model format. For inputting models generated through the application, simple path to the output .keras.zip file downloaded from a previous training event.\n\nIf ‘training’ is selected, the selected dataset(s) will function as training data.\nIf ‘inference’ is selected, the selected dataset(s) will function as data upon which inference is applied (output statistics will use existance of ‘age’ if available, otherwise will report histogram of output ages)\nIf ‘fine-tuning’ is selected, the selected dataset(s) will function as data for which the model is fine tuned on.\n\n\n\nData Columns",
    "crumbs": [
      "Use",
      "The app"
    ]
  },
  {
    "objectID": "content/dataPrep.html",
    "href": "content/dataPrep.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Examples",
      "Data preparation"
    ]
  },
  {
    "objectID": "content/useCodebase.html",
    "href": "content/useCodebase.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Use",
      "The codebase"
    ]
  },
  {
    "objectID": "content/futDirections.html",
    "href": "content/futDirections.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Future Directions"
    ]
  },
  {
    "objectID": "content/exApp.html",
    "href": "content/exApp.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Examples",
      "The app"
    ]
  },
  {
    "objectID": "content/Maintaining.html",
    "href": "content/Maintaining.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Maintaining"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fourier transform near-infrared spectroscopy / machine learning otolith aging application",
    "section": "",
    "text": "The AFSC Age and Growth Program have pioneered machine learning methods to streamline specimen age predictions from otoliths. The approach is a multimodal convolutional neural network, (MM- CNN) as described in Benson et al. (2019), which encorporates two key data modalities: the entire range of wavenumbers of FT-NIR spectra, and corresponding biological and geospatial data.\nDuring development of this technique, scripts was developed in jupyter notebooks and demonstrated excellent performance in multiple species age prediction (cite). To bring these techniques to a wider audience, the code was refactored into functional programming. The core scripts were refactored into a functional programming codebase that encorporated ideas like data standards and the machine learning lifecycle to promote reuse and extention of the method across a broader range of data and machine learning techniques.\nThis codebase was developed in python, and currently supports the following features:\n\nFull machine learning lifecycle:\n\nTraining\nInference\nFine-tuning\n\nModel inference and fine-tuning will use the data given as best possible\n\nAutomatic interpolation of spectra as required\nNew data can be incorporated into fine tuning while preserving existing model weights\n\nBuilt in features for under-the-hood data formatting\n\nautomatic one hot encoding for known categorical variables\nawareness of standard variables to promote combining data where appropriate\n\n\nThe application serves to deliver the codebase core logic in an easy to use interface.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "Fourier transform near-infrared spectroscopy / machine learning otolith aging application",
    "section": "",
    "text": "The AFSC Age and Growth Program have pioneered machine learning methods to streamline specimen age predictions from otoliths. The approach is a multimodal convolutional neural network, (MM- CNN) as described in Benson et al. (2019), which encorporates two key data modalities: the entire range of wavenumbers of FT-NIR spectra, and corresponding biological and geospatial data.\nDuring development of this technique, scripts was developed in jupyter notebooks and demonstrated excellent performance in multiple species age prediction (cite). To bring these techniques to a wider audience, the code was refactored into functional programming. The core scripts were refactored into a functional programming codebase that encorporated ideas like data standards and the machine learning lifecycle to promote reuse and extention of the method across a broader range of data and machine learning techniques.\nThis codebase was developed in python, and currently supports the following features:\n\nFull machine learning lifecycle:\n\nTraining\nInference\nFine-tuning\n\nModel inference and fine-tuning will use the data given as best possible\n\nAutomatic interpolation of spectra as required\nNew data can be incorporated into fine tuning while preserving existing model weights\n\nBuilt in features for under-the-hood data formatting\n\nautomatic one hot encoding for known categorical variables\nawareness of standard variables to promote combining data where appropriate\n\n\nThe application serves to deliver the codebase core logic in an easy to use interface.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "content/contScience.html",
    "href": "content/contScience.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Contributing",
      "Evolving scientific techniques"
    ]
  },
  {
    "objectID": "content/Examples.html",
    "href": "content/Examples.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Examples"
    ]
  },
  {
    "objectID": "content/Contributing.html",
    "href": "content/Contributing.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "content/contDocumentation.html",
    "href": "content/contDocumentation.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Contributing",
      "The documentation"
    ]
  },
  {
    "objectID": "content/backDesign.html",
    "href": "content/backDesign.html",
    "title": "Application design",
    "section": "",
    "text": "To bring the scientific technique into operational state, the AFSC Age & Growth program began work with AFSC OFIS in spring of 2024 to make the work more accessible to non-technical users. The core design of any solution involved:\n\nA python-based ML codebase that contained functions encapsulating the scientific processes\nSoftware that involved delivery of the codebase in an accessible format\n\nAFSC Age & Growth program socialized this idea with other science center researchers and NMFS IT community members, before defining the initial concept.\n\n\n\nOriginal products concept, March 2024",
    "crumbs": [
      "Background",
      "The design"
    ]
  },
  {
    "objectID": "content/backDesign.html#concept",
    "href": "content/backDesign.html#concept",
    "title": "Application design",
    "section": "",
    "text": "To bring the scientific technique into operational state, the AFSC Age & Growth program began work with AFSC OFIS in spring of 2024 to make the work more accessible to non-technical users. The core design of any solution involved:\n\nA python-based ML codebase that contained functions encapsulating the scientific processes\nSoftware that involved delivery of the codebase in an accessible format\n\nAFSC Age & Growth program socialized this idea with other science center researchers and NMFS IT community members, before defining the initial concept.\n\n\n\nOriginal products concept, March 2024",
    "crumbs": [
      "Background",
      "The design"
    ]
  },
  {
    "objectID": "content/backDesign.html#software-stack",
    "href": "content/backDesign.html#software-stack",
    "title": "Application design",
    "section": "Software stack",
    "text": "Software stack\n\nOperating system:\nBoth codebases are 100% python based, and are cross OS compatible. The application and the ML codebase can, and are, run on both Windows and Linux hosts.\n\n\nPython:\nPython is popular programming language with many useful packages for the underlying processes:\n\nML codebase core packages:\n\nWorking with data tables (pandas)\nMachine learning (tensorflow + keras)\n\nApplication core packages:\n\nApplication front-end (Dash)\nData and machine learning backend (ML codebase)\n\n\n\n\nPersistent data:\nThe application itself is stateless, meaning a new hosted copy of the application will perform all needed functions. The application uses Google Cloud Storage on the backend to store persistent data. The application authenticates to Google credentials to communicate with the storage location, and so requires a Google Cloud Project, and the application needs to have access to personal credentials (when running for individual use) or a service account (when running for shared use).\nAFSC OFIS/Age and Growth are currently hosting site assets (the server, and the backend cloud storage), on a NOAA GCP organization cloud project belonging to Age and Growth and adminstrated by AFSC OFIS. A hardened (meaning, AFSC secure) Ubuntu Pro image hosts the application in a docker compose stack, and uses NGINX for publication to an NMFS internal https port.",
    "crumbs": [
      "Background",
      "The design"
    ]
  },
  {
    "objectID": "content/exCodebase.html",
    "href": "content/exCodebase.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Examples",
      "The codebase"
    ]
  },
  {
    "objectID": "content/Background.html",
    "href": "content/Background.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "content/maintApp.html",
    "href": "content/maintApp.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Maintaining",
      "App data and models"
    ]
  },
  {
    "objectID": "content/selfHost.html",
    "href": "content/selfHost.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Maintaining",
      "Self-hosting"
    ]
  },
  {
    "objectID": "content/Use.html",
    "href": "content/Use.html",
    "title": "Test page",
    "section": "",
    "text": "contents",
    "crumbs": [
      "Use"
    ]
  }
]